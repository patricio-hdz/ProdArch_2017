1 Abstract  (Todos domingo)

-	Máximo 2 párrafos.
-	 This article shows a way to build a recommendation system for new users encountering a large chain of mails …..
-	Mencionar el uso de LDA
-	Mencionar que se utilizó docker, luigi, spark y aws sin el detalle.

2 Introduction (Sin cambios y mañana lo revisamos)

2.1 Main Goal (Max)
-	Exponer la idea de encontrar los temas principales del extropy chat.
-	Exponer la idea de encontrar los actores principales del extropy chat.
-	Externar la necesidad de implementar un producto de datos replicable, portable y escalable.

2.2 Data Stories (Max)
-	Reformular las data stories para planteralas de tal forma que contextualicemos los dos primeros main goals, es decir, que primero buscabamos maneras de obtener respuestas a dichos goals y finalmente le encontramos un uso en recomendaciones.

-	No mencionar los data stories originales ya que varios de esos temas no se resolvieron.

3 NUEVO CHAPTER “Modelos” (Alex)

-	Explicación de los modelos, motivación, necesidades, medidas de ajuste, otros.
-	Esta sección servirá como motivación de las decisiones que se tomarón tanto para el pipeline como para la arquitectura.

4 Pipeline (Todos mañana)

-	Revisar la cantidad de pasos del pipeline:
-	Actualizar los códigos usados.
- 	Editar los códigos con los finales () (Esto hacerlo al final porque todavía podemos terminar algunas cosas pendientes)

4.1 Luigi (Daniel)

- Por qué Luigi.
- Código.
- Pipeline ejecutado mediante código.

4.2 Extropy Chat-Raw Data (Daniel)

-	Mencionar hasta nuestra útlima descarga cuántos archivos obtuvimos y en MB.

4.3 Raw Data – Clean Data Set (Cambiar el título a algo estilo: Divide y venceras) (Daniel)

-	Se juntan los archivos mensuales en archivos por año para disminuir el posible error en el parseo.

-	Mencionar que se inicio con # cantidad de archivos y se obtuvieron #2 cantidad de archivos (para facilitar al lector, así lo vimos en un par de artículos jaja).

4.4 Clean Data Set – Json (Daniel)

-	La finalidad de obtener 1 JSON es tener un “algo” a lo cual podamos tirarle queries.
-	Mencionar que se inicia con #2 cantidad de archivos y se obtiene finalmente 1 archivo JSON.



Eliminar 4.4 JSON – CSV 

-	Eliminar este paso del pipeline.

4.5 JSON-txts by Subject (Daniel)

- Mencionar como se tranformo la idea de ir a un CSV hasta lo que terminamos haciendo.

-	Mencionar cuántos Subjects tenemos que es equivalente al número de txts obtenidos.

-	Mencionar que tanto esos txts, el JSON, y un archivo llamado stopwords se envían al S3.

- 	Poner una nota acerca de en qué consiste el archivo stopwords, que esencialmente son palabras que generan problemas los modelos de análisis de textos.

4.6 Implementación de Modelos (Alex)

-	Explicar el código de spark.
- 	Mencionar tiempos de ejecución, mejoras en performance gracias al cluster, etc.
-	Mencionar que escribe al S3.

4.7 Shiny Dashboard (Patricio)

-	Explicar código de R.
- 	Obtención de los datos.
-	Explicar decisión del tipo de dashboard.


5 Data-product-architecture (Max)

-	Reforzar ideas de portabilidad, escalabilidad y portabilidad.
5.1 Architecture tools (antes Description of the architecture) (Max)

-	Mencionar el mótivo del uso de docker, luigi, aws, spark.

5.2 Pipeline based Architecture (Daniel)

-	Mencionar cómo está armada la arquitectura en cada paso del pipeline, incluyendo el detalle del uso de Luigi para orquestar dichos pasos. Por ejemplo; Descarga: se genera un docker para el cual se publican las imágenes bla bla, (mostrar código) y explicar cómo dicho código permite llevar a cabo el siguiente paso.

-	Mencionar costos de maquinas.

-	Explicar decisiones tomadas para cada paso parte de la arquitectura.


6.0 Results (Patricio y Alex borrador - Todos mañana)

-	Mencionar los resultados del prelyminary analysis.
-	Mencionar los resultados del spark.
-	Responder los goals planteados al inicio.
-	Logros en la arquitectura ¿portable, escalable y replicable?
-	Interpretación de modelos y cómo terminamos usando los resultados para recomendaciones.
-	Mejoras en cuanto arquitectura.
-	Otros cuestionamientos que surgen de los datos y no se abarcan en el producto.

7.0 Conclusiones (Todos)
-	Qué aprendimos: importancia de la escalabilidad, paralelismo, orquestación y desarrollo de productos de datos(desde la planeación hasta implementacion) con un equipo de trabajo.
-	Visualización de los resultados de tal manera que sean entendibles para usuarios no técnicos.
- 	Implantación de los resultados en un ejemplo real.

